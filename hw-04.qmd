---
title: "hw-04"
author: "VISHAL BHASHAAM"
format: html
editor: visual
---

## Big tech stocks

### Question: 

How do daily opening prices, trading volumes, and historical trends influence the adjusted closing prices of stocks?

### Description:

This dataset consists of the daily stock prices and volume of 14
different tech companies, including Apple (AAPL), Amazon (AMZN),
Alphabet (GOOGL), and Meta Platforms (META) and more!

# Regression in r

Regression is a modeling technique for predicting quantitative-valued target attributes. Works on the principle of `y = mx + c` .

```{r}
# Reading the CSV
stock<- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-07/big_tech_stock_prices.csv')


stocks <- stock 
```

```{r}
stock |> head()
```

## Loading Required packages

```{r}
# Required packages
if (!require(pacman))
  install.packages("pacman")

pacman::p_load(tidymodels,
               tidyverse,
               ranger,
               randomForest,
               glmnet,
               gridExtra)

# Global ggplot theme
theme_set(theme_bw() + theme(legend.position = "top"))
```

## Stock data - assigning values to X and y (input and target variable )

```{r,warning=FALSE}
# Parameters 
seed <- 1
numInstances <- nrow(stock) 

# set seed
set.seed(seed)

# Generate data 
X <- stock$open
y_true <- stock$close
y <- y_true + matrix(rnorm(numInstances), ncol =1)
# adding noise to the data

# Plot 
ggplot(stock,aes(x=X, y=y)) +
 geom_point(color = "red") +
  geom_smooth(method = "lm",color = "black", linewidth = 1) +
  ggtitle("Stock dataset open and close price relationship") +
  
  theme_classic()+
  xlab("Open price")+
  ylab('close price')
```

-   The above plots depicts that when the open price increases the closing price also increases and the data looks to be linear.

## Multiple linear regression

Multiple linear regression is a statistical method used to model the relationship between a dependent variable and two or more independent variables.

Step 1: split input data into training and test sets:

```{r}
# Train/test split
library(rsample)

numTrain <- 100
numTest <-numInstances - numTrain

set.seed(123)

data <- tibble(X=X, y=y)

split_obj <-initial_split(data, prop = numTrain/numInstances)

# Extract train and test data 

train_data <- training(split_obj)
test_data <-  testing(split_obj)

# Extract X_train, X_test , y_train, y_test

X_train <-train_data$X
y_train <-train_data$y

X_test <- test_data$X
y_test <- test_data$y
```

-   Splitting the data into train and test data.

Step 2: Fit regression model to Training Set

```{r,warning=FALSE}
# Create a linear regression model specification 
library(parsnip)

lin_ref_spec <-  linear_reg() |>

  set_engine("lm")

# Fit the model to the training data

lin_reg_fit <-lin_ref_spec |>
  fit(y~ X, data = train_data)
```

-   We are fitting the model using the train data where the X is features (`open` ) and y is the `close` price.

Step 3: Apply Model to the Test Set

```{r}
# Apply model to the test set 
y_pred_test <- predict(lin_reg_fit, new_data =test_data) |>
  pull(.pred)
```

-   Exposing model to the new data (test data) to check how generalized the model performs.

Step 4: Evaluate Model Performance on the test set

```{r}
# Plotting true vs ppredicted values

ggplot() +
   geom_point(aes(x= as.vector(y_test), y= y_pred_test), color ="red")+
  ggtitle("Comparing true and predicted variable for the test set")+
  xlab("True values for y")+
  ylab("Predicted values for y")

```

-   From the above plot we can that the predicted values are close to the true values which says the model has performed good and it is linear, still there is room for improvement.

```{r}
library(yardstick)

# Prepare data for yardstick evaluation
truth  <-  as.vector(y_test)
estimate <- y_pred_test
eval_data <- tibble (
  truth  = as.vector(y_test),
  estimate = y_pred_test
)

# Model evaluation

rmse_value <- mean((truth - estimate)^2) %>% sqrt()
r2_value <- rsq(eval_data, truth = truth, estimate = estimate)

cat("Root mean squared error =", sprintf("%.4f",rmse_value),"\n")

```

-   The `RMSE` value suggest that how foar each point in the data is far away from the true value.

-   2.8092 is relatively low value, so the predictions has less amount of deviation from the true value.

```{r}
cat('R-squared =', sprintf("%.4f",r2_value$.estimate),"\n")
```

-   `R-Squared` = 0.9993 is high value, which means that the model has a good fit on the data and works well newly exposed data as well.

Step 5: Postprocessing

```{r}
# Display model parameters

coef_values <- coef(lin_reg_fit$fit) # Extract Coefficients
slope <- coef_values["X"]
intercept <- coef_values["(Intercept)"]

cat ("Slope =", intercept,"\n")
```

-   The slope of the variables is positive thsi means that two variables are related and when `X` increases so does `y`.

```{r}
### Step 4: Preprocessing

# Plot Outputs
ggplot() +
  geom_point(aes(x = as.vector(X_test), y = as.vector(y_test )),color = "black")+ 
  geom_line(aes(x= as.vector(X_test), y = y_pred_test), color = "blue",linewidth = 1)+
  ggtitle(sprintf('Predicted function: y = %.2fX + %.2f', slope, intercept)) +
   xlab('X') +
  ylab('y')
```

## Effect of correlated attributes

Finding out how each feature responds to one another: (for example:when one increases other also does or simple words is cause and effect);

Assigning the correlated attributes: (`High, low , adj_close , volume`):

```{r}
# mentioning the other features in the data
set.seed(1)
X2 <- stocks$high
X3 <- stocks$low
X4 <- stocks$adj_close
X5 <- stocks$volume

# Create plots
plot1 <- ggplot() +
  geom_point(aes(X, X2), color='black') +
  xlab('X') + ylab('X2') +
  ggtitle(sprintf("Correlation between X and X2 = %.4f", cor(X[-c((numInstances-numTest+1):numInstances)], X2[-c((numInstances-numTest+1):numInstances)])))

plot2 <- ggplot() +
  geom_point(aes(X2, X3), color='black') +
  xlab('X2') + ylab('X3') +
  ggtitle(sprintf("Correlation between X2 and X3 = %.4f", cor(X2[-c((numInstances-numTest+1):numInstances)], X3[-c((numInstances-numTest+1):numInstances)])))

plot3 <- ggplot() +
  geom_point(aes(X3, X4), color='black') +
  xlab('X3') + ylab('X4') +
  ggtitle(sprintf("Correlation between X3 and X4 = %.4f", cor(X3[-c((numInstances-numTest+1):numInstances)], X4[-c((numInstances-numTest+1):numInstances)])))

plot4 <- ggplot() +
  geom_point(aes(X4, X5), color='black') +
  xlab('X4') + ylab('X5') +
  ggtitle(sprintf("Correlation between X4 and X5 = %.4f", cor(X4[-c((numInstances-numTest+1):numInstances)], X5[-c((numInstances-numTest+1):numInstances)])))

# Combine plots into a 2x2 grid
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

-   Plot1: X and X2 has strong correlation as x increases y also increases.

-   Plot2: X2 and X3 has strong correlation as X and X2.

-   Plot 3: X3 and X4 also has strong correlation but has some point of deviation at some point of interval as seen above.

-   Plot 4: X4 and X5 are independent of each other as the graph is not linear so it has a weak correlation.

```{r}
# Split data into training and testing sets
train_indices <- 1:(numInstances - numTest)
test_indices <- (numInstances - numTest + 1):numInstances

# Create combined training and testing sets
X_train2 <- cbind(X[train_indices], X2[train_indices])
X_test2 <- cbind(X[test_indices], X2[test_indices])

X_train3 <- cbind(X[train_indices], X2[train_indices], X3[train_indices])
X_test3 <- cbind(X[test_indices], X2[test_indices], X3[test_indices])

X_train4 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices])
X_test4 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices])

X_train5 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices], X5[train_indices])
X_test5 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices], X5[test_indices])
```

-   Again splitting the data into train and test sets for newly created distribution with new features.

```{r}
# Convert matrices to tibbles for training
train_data2 <- tibble(X1 = X_train2[,1], X2 = X_train2[,2], y = y_train)
train_data3 <- tibble(X1 = X_train3[,1], X2 = X_train3[,2], X3 = X_train3[,3], y = y_train)
train_data4 <- tibble(X1 = X_train4[,1], X2 = X_train4[,2], X3 = X_train4[,3], X4 = X_train4[,4], y = y_train)
train_data5 <- tibble(X1 = X_train5[,1], X2 = X_train5[,2], X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5], y = y_train)

# Train models
regr2_spec <- linear_reg() %>% set_engine("lm")
regr2_fit <- regr2_spec %>% fit(y ~ X1 + X2, data = train_data2)

regr3_spec <- linear_reg() %>% set_engine("lm")
regr3_fit <- regr3_spec %>% fit(y ~ X1 + X2 + X3, data = train_data3)

regr4_spec <- linear_reg() %>% set_engine("lm")
regr4_fit <- regr4_spec %>% fit(y ~ X1 + X2 + X3 + X4, data = train_data4)

regr5_spec <- linear_reg() %>% set_engine("lm")
regr5_fit <- regr5_spec %>% fit(y ~ X1 + X2 + X3 + X4 + X5, data = train_data5)
```

-   Fitting different linear models with different set of features.

-   This ensures which features works best to predict the target.

```{r}
# Convert matrices to data.frames for predictions
new_train_data2 <- setNames(as.data.frame(X_train2), c("X1", "X2"))
new_test_data2 <- setNames(as.data.frame(X_test2), c("X1", "X2"))

new_train_data3 <- setNames(as.data.frame(X_train3), c("X1", "X2", "X3"))
new_test_data3 <- setNames(as.data.frame(X_test3), c("X1", "X2", "X3"))

new_train_data4 <- setNames(as.data.frame(X_train4), c("X1", "X2", "X3", "X4"))
new_test_data4 <- setNames(as.data.frame(X_test4), c("X1", "X2", "X3", "X4"))

new_train_data5 <- setNames(as.data.frame(X_train5), c("X1", "X2", "X3", "X4", "X5"))
new_test_data5 <- setNames(as.data.frame(X_test5), c("X1", "X2", "X3", "X4", "X5"))

# Predictions
y_pred_train2 <- predict(regr2_fit, new_data = new_train_data2)
y_pred_test2 <- predict(regr2_fit, new_data = new_test_data2)

y_pred_train3 <- predict(regr3_fit, new_data = new_train_data3)
y_pred_test3 <- predict(regr3_fit, new_data = new_test_data3)

y_pred_train4 <- predict(regr4_fit, new_data = new_train_data4)
y_pred_test4 <- predict(regr4_fit, new_data = new_test_data4)

y_pred_train5 <- predict(regr5_fit, new_data = new_train_data5)
y_pred_test5 <- predict(regr5_fit, new_data = new_test_data5)
```

-   Exposing the created models with different set of features to test set, so that we can find the error and accuracy of the model predictions.

```{r}
# Extract coefficients and intercepts
get_coef <- function(model) {
  coef <- coefficients(model$fit)
  coef
}

# Calculate RMSE
calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)^2))
  rmse
}

results <- tibble(
  Model = c(sprintf("%.2f X + %.2f", get_coef(regr2_fit)['X1'], get_coef(regr2_fit)['(Intercept)']),
            sprintf("%.2f X + %.2f X2 + %.2f", get_coef(regr3_fit)['X1'], get_coef(regr3_fit)['X2'], get_coef(regr3_fit)['(Intercept)']),
            sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f", get_coef(regr4_fit)['X1'], get_coef(regr4_fit)['X2'], get_coef(regr4_fit)['X3'], get_coef(regr4_fit)['(Intercept)']),
            sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f", get_coef(regr5_fit)['X1'], get_coef(regr5_fit)['X2'], get_coef(regr5_fit)['X3'], get_coef(regr5_fit)['X4'], get_coef(regr5_fit)['(Intercept)'])),
  
  Train_error = c(calculate_rmse(y_train, y_pred_train2$.pred),
                  calculate_rmse(y_train, y_pred_train3$.pred),
                  calculate_rmse(y_train, y_pred_train4$.pred),
                  calculate_rmse(y_train, y_pred_train5$.pred)),
  
  Test_error = c(calculate_rmse(y_test, y_pred_test2$.pred),
                 calculate_rmse(y_test, y_pred_test3$.pred),
                 calculate_rmse(y_test, y_pred_test4$.pred),
                 calculate_rmse(y_test, y_pred_test5$.pred)),
  
  Sum_of_Absolute_Weights = c(sum(abs(get_coef(regr2_fit))),
                              sum(abs(get_coef(regr3_fit))),
                              sum(abs(get_coef(regr4_fit))),
                              sum(abs(get_coef(regr5_fit))))
)

# Plotting
ggplot(results, aes(x = Sum_of_Absolute_Weights)) +
  geom_line(aes(y = Train_error, color = "Train error"), linetype = "solid") +
  geom_line(aes(y = Test_error, color = "Test error"), linetype = "dashed") +
  labs(x = "Sum of Absolute Weights", y = "Error rate") +
  theme_minimal()
```

```{r}
results 
```

-   Sum of Absolute weights show the model complexity, as you can with our predictions, as the number of features goes higher the model complexity also increases.

-   As you can see, the train error for the model is less compared to the test error which on a higher rate compared. This means that model has overfitting issues (works well on the train data and not the testing data) and works bad for new data.

-   To avoid this issue, we can use regularization (Induce penalty for the model to correct the errors)

## Ridge Regression

Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of
multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.

```{r}
# Convert to data frame
train_data <- tibble(y = y_train, X_train5)
test_data <- tibble(y = y_test, X_test5)

# Set up a Ridge regression model specification
ridge_spec <- linear_reg(penalty = 0.4, mixture = 1) %>% 
  set_engine("glmnet")

# Fit the model
ridge_fit <- ridge_spec %>% 
  fit(y ~ ., data = train_data)

# Make predictions
y_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = test_data)$.pred


# Make predictions
y_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = train_data)$.pred

# Calculate RMSE
calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)^2))
  rmse
}

# Extract coefficients
ridge_coef <- coefficients(ridge_fit$fit)

model6 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                 ridge_coef[2], ridge_coef[3], ridge_coef[4], 
                 ridge_coef[5], ridge_coef[6], ridge_coef[1])

values6 <- tibble(
  Model = model6,
  Train_error = calculate_rmse(y_train, y_pred_train_ridge),
  Test_error = calculate_rmse(y_test, y_pred_test_ridge),
  Sum_of_Absolute_Weights = sum(abs(ridge_coef))
)

# Combining the results
final_results <- bind_rows(results, values6)

final_results
```

-   As you can see the Ridge regression worked best when there a lot of features used, the last model with 5 features open, high, low, adj_close, volume.

-   The test error has decreased drastically when compared to the other models where was no improvement.

-   The tradeofff here is that the test error has increased but the sum of the weights or the complexity of the model has also increased.

-   But we can that most of the coefficients has been reduced to zero to generalize to the unseen data. Therefore Ridge regression has some progress in reducing the test error rate.

## Lasso regression

LASSO regression, also known as L1 regularization, is a popular technique used in statistical modeling and machine learning to estimate the relationships between variables and make predictions. LASSO stands for Least Absolute Shrinkage and Selection Operator. The primary goal of LASSO regression is to find a balance between model simplicity and accuracy. It achieves this by adding a penalty term to the traditional linear regression model, which encourages sparse solutions where some coefficients are forced to be exactly zero.

```{r}
# Define the lasso specification
lasso_spec <- linear_reg(penalty = 0.02, mixture = 1) %>% 
  set_engine("glmnet")

# Ensure the data is combined correctly
train_data <- tibble(y = y_train, X1 = X_train5[,1], X2 = X_train5[,2], 
                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])

# Fit the model
lasso_fit <- lasso_spec %>%
  fit(y ~ ., data = train_data)

# Extract coefficients
lasso_coefs <- lasso_fit$fit$beta[,1]

# Predictions
y_pred_train_lasso <- predict(lasso_fit, new_data = train_data)$.pred
y_pred_test_lasso <- predict(lasso_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], 
                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred

# Create the model string
model7 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                  lasso_coefs[2], lasso_coefs[3], lasso_coefs[4], 
                  lasso_coefs[5], lasso_coefs[6], lasso_fit$fit$a0[1])

values7 <- c(model7, 
             sqrt(mean((y_train - y_pred_train_lasso)^2)),
             sqrt(mean((y_test - y_pred_test_lasso)^2)),
             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))

# Make the results tibble
lasso_results <- tibble(Model = "Lasso",
                        `Train error` = values7[2], 
                        `Test error` = values7[3], 
                        `Sum of Absolute Weights` = values7[4])

lasso_results
```

-   We can that lasso has no improvement on train or test error instead it has reduced the complexity of the model by assigning coefficient to zero and making a improvement on sum of absolute weights.

-   Therefore we can conclude lasso regression was no effective for this dataset.

## Hyperparameter Selection via Cross-validation

```{r}
# Combine training data
y_train <- as.vector(y_train)

train_data <- tibble(y = y_train, X1 = X_train5[,1], X2 = X_train5[,2], 
                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])

# Define recipe
recipe_obj <- recipe(y ~ ., data = train_data) %>%
  step_normalize(all_predictors()) |>
  prep()

# Define the ridge specification
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>% 
  set_engine("glmnet")

# Ridge workflow
ridge_wf <- workflow() |>
  add_model(ridge_spec) |>
  add_recipe(recipe_obj)

# Grid of alphas
alphas <- tibble(penalty = c(0.2, 0.4, 0.6, 0.8, 1.0))

# Tune
tune_results <- 
  ridge_wf |>
  tune_grid(
  resamples = bootstraps(train_data, times = 5),
  grid = alphas
)


# Extract best parameters
best_params <- tune_results %>% select_best("rmse")

# Refit the model
ridge_fit <- ridge_spec %>%
  finalize_model(best_params) %>%
  fit(y ~ ., data = train_data)

# Extract coefficients
ridge_coefs <- ridge_fit$fit$beta[,1]

# Predictions
y_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], 
                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred

# Create the model string
model6 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                  ridge_coefs[2], ridge_coefs[3], ridge_coefs[4], 
                  ridge_coefs[5], ridge_coefs[6], ridge_fit$fit$a0[1])

values6 <- c(model6, 
             sqrt(mean((y_train - y_pred_train_ridge)^2)),
             sqrt(mean((y_test - y_pred_test_ridge)^2)),
             sum(abs(ridge_coefs[-1])) + abs(ridge_fit$fit$a0[1]))

# Make the results tibble
ridge_results <- tibble(Model = "RidgeCV",
                        `Train error` = values6[2], 
                        `Test error` = values6[3], 
                        `Sum of Absolute Weights` = values6[4])

cat("Selected alpha =", best_params$penalty, "\n")
```

```{r}
all_results <- bind_rows(results, ridge_results)
all_results
```

-   Compared to the older results, hyper parameter selection has increased the test error, which doesn't generalize the model, it worked similar to lasso in terms of reducing the complexity of the model.

```{r}
set.seed(1234)

# Ensure y_train is a vector
y_train <- as.vector(y_train)

# Combine training data
train_data <- tibble(y = y_train, X1 = X_train5[,1], X2 = X_train5[,2], 
                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])

# Define recipe
recipe_obj_lasso <- recipe(y ~ ., data = train_data) %>%
  step_normalize(all_predictors()) |>
  prep()

# Define the lasso specification
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

# Lasso workflow
lasso_wf <- workflow() |>
  add_recipe(recipe_obj_lasso)

# Lasso fit
lasso_fit <- lasso_wf |>
  add_model(lasso_spec) |>
  fit(data = train_data)

# Grid of alphas for Lasso
lambda_grid <- grid_regular(penalty(), levels = 50)

# Tune
tune_results_lasso <- 
  tune_grid(lasso_wf |> add_model(lasso_spec),
  resamples = bootstraps(train_data, times = 5),
  grid = lambda_grid
)

# Extract best parameters for Lasso
best_params_lasso <- tune_results_lasso %>% select_best("rmse")

# Refit the model using Lasso
lasso_fit <- lasso_spec %>%
  finalize_model(best_params_lasso) %>%
  fit(y ~ ., data = train_data)

# Extract coefficients
lasso_coefs <- lasso_fit$fit$beta[,1]

# Predictions using Lasso
y_pred_train_lasso <- predict(lasso_fit, new_data = train_data)$.pred
y_pred_test_lasso <- predict(lasso_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], 
                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred

# Create the model string for Lasso
model7 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                  lasso_coefs[2], lasso_coefs[3], lasso_coefs[4], 
                  lasso_coefs[5], lasso_coefs[6], lasso_fit$fit$a0[1])

values7 <- c(model7, 
             sqrt(mean((y_train - y_pred_train_lasso)^2)),
             sqrt(mean((y_test - y_pred_test_lasso)^2)),
             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))

# Make the results tibble for Lasso
lasso_results <- tibble(Model = "LassoCV",
                        `Train error` = values7[2], 
                        `Test error` = values7[3], 
                        `Sum of Absolute Weights` = values7[4])

cat("Selected alpha for Lasso =", best_params_lasso$penalty, "\n")
```

```{r}
lasso_results
```

-   We can see that there is no improvement in the model after hyperparameter selection.

-   The old lasso regression and new results are the same.

## Answer to the question:

### How do daily opening prices, trading volumes, and historical trends influence the adjusted closing prices of stocks?

-   Comparing to mulitple models created in this assignment, I can conclude that using open prices to predict the closing price does a better a job in predicting the target.

-   Multiple regression did not work better on this dataset.
